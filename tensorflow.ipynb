{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "owned-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from IPython.display import Image\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-render",
   "metadata": {},
   "source": [
    "#### 問題1\n",
    "\n",
    "- 重み、バイアスの初期化、勾配計算、更新\n",
    "- 層の定義(何層用意するか、パラメータ設定)\n",
    "- 損失関数計算\n",
    "- 正解率計算\n",
    "- 学習率設定, Adagard実装\n",
    "- 入力Xに対する予測\n",
    "- 試行回数(epoch)のloop処理\n",
    "- バッチで入力を分割して学習\n",
    "- 活性関数定義\n",
    "- フォワードプロパゲージョン、バックプロパゲージョンの実装\n",
    "- 学習過程をdebug出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "checked-broadcasting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 7.0241, val_loss : 67.6859, acc : 0.375\n",
      "Epoch 1, loss : 3.4241, val_loss : 23.4026, acc : 0.312\n",
      "Epoch 2, loss : 1.9387, val_loss : 11.6681, acc : 0.375\n",
      "Epoch 3, loss : 2.0917, val_loss : 13.1400, acc : 0.312\n",
      "Epoch 4, loss : 1.7685, val_loss : 17.7284, acc : 0.312\n",
      "Epoch 5, loss : 1.6097, val_loss : 12.9607, acc : 0.312\n",
      "Epoch 6, loss : 1.4402, val_loss : 10.0593, acc : 0.312\n",
      "Epoch 7, loss : 1.3704, val_loss : 9.4797, acc : 0.312\n",
      "Epoch 8, loss : 1.2536, val_loss : 9.8518, acc : 0.312\n",
      "Epoch 9, loss : 1.1476, val_loss : 8.5670, acc : 0.375\n",
      "Epoch 10, loss : 1.0930, val_loss : 8.0430, acc : 0.375\n",
      "Epoch 11, loss : 1.0412, val_loss : 7.8791, acc : 0.375\n",
      "Epoch 12, loss : 0.9804, val_loss : 7.1233, acc : 0.375\n",
      "Epoch 13, loss : 0.9326, val_loss : 6.7908, acc : 0.375\n",
      "Epoch 14, loss : 0.8792, val_loss : 6.2492, acc : 0.375\n",
      "Epoch 15, loss : 0.8304, val_loss : 5.7680, acc : 0.375\n",
      "Epoch 16, loss : 0.7835, val_loss : 5.2886, acc : 0.438\n",
      "Epoch 17, loss : 0.7384, val_loss : 4.8037, acc : 0.438\n",
      "Epoch 18, loss : 0.6961, val_loss : 4.3575, acc : 0.500\n",
      "Epoch 19, loss : 0.6543, val_loss : 3.9175, acc : 0.500\n",
      "Epoch 20, loss : 0.6136, val_loss : 3.5188, acc : 0.500\n",
      "Epoch 21, loss : 0.5738, val_loss : 3.1372, acc : 0.500\n",
      "Epoch 22, loss : 0.5349, val_loss : 2.7697, acc : 0.500\n",
      "Epoch 23, loss : 0.4973, val_loss : 2.4528, acc : 0.562\n",
      "Epoch 24, loss : 0.4606, val_loss : 2.1728, acc : 0.562\n",
      "Epoch 25, loss : 0.4255, val_loss : 1.9324, acc : 0.625\n",
      "Epoch 26, loss : 0.3919, val_loss : 1.7049, acc : 0.625\n",
      "Epoch 27, loss : 0.3609, val_loss : 1.5248, acc : 0.688\n",
      "Epoch 28, loss : 0.3326, val_loss : 1.3725, acc : 0.750\n",
      "Epoch 29, loss : 0.3071, val_loss : 1.2386, acc : 0.750\n",
      "Epoch 30, loss : 0.2851, val_loss : 1.1454, acc : 0.750\n",
      "Epoch 31, loss : 0.2647, val_loss : 1.0364, acc : 0.750\n",
      "Epoch 32, loss : 0.2466, val_loss : 0.9368, acc : 0.750\n",
      "Epoch 33, loss : 0.2297, val_loss : 0.8304, acc : 0.750\n",
      "Epoch 34, loss : 0.2155, val_loss : 0.7243, acc : 0.750\n",
      "Epoch 35, loss : 0.2024, val_loss : 0.6215, acc : 0.750\n",
      "Epoch 36, loss : 0.1920, val_loss : 0.5405, acc : 0.812\n",
      "Epoch 37, loss : 0.1815, val_loss : 0.4515, acc : 0.812\n",
      "Epoch 38, loss : 0.1735, val_loss : 0.4015, acc : 0.812\n",
      "Epoch 39, loss : 0.1643, val_loss : 0.3323, acc : 0.812\n",
      "Epoch 40, loss : 0.1574, val_loss : 0.2935, acc : 0.875\n",
      "Epoch 41, loss : 0.1496, val_loss : 0.2519, acc : 0.875\n",
      "Epoch 42, loss : 0.1429, val_loss : 0.2206, acc : 0.875\n",
      "Epoch 43, loss : 0.1364, val_loss : 0.1932, acc : 0.875\n",
      "Epoch 44, loss : 0.1302, val_loss : 0.1684, acc : 0.875\n",
      "Epoch 45, loss : 0.1244, val_loss : 0.1464, acc : 0.875\n",
      "Epoch 46, loss : 0.1188, val_loss : 0.1264, acc : 0.875\n",
      "Epoch 47, loss : 0.1139, val_loss : 0.1098, acc : 0.875\n",
      "Epoch 48, loss : 0.1091, val_loss : 0.0946, acc : 0.938\n",
      "Epoch 49, loss : 0.1050, val_loss : 0.0833, acc : 0.938\n",
      "Epoch 50, loss : 0.1007, val_loss : 0.0712, acc : 1.000\n",
      "Epoch 51, loss : 0.0975, val_loss : 0.0653, acc : 1.000\n",
      "Epoch 52, loss : 0.0932, val_loss : 0.0531, acc : 1.000\n",
      "Epoch 53, loss : 0.0916, val_loss : 0.0558, acc : 1.000\n",
      "Epoch 54, loss : 0.0862, val_loss : 0.0390, acc : 1.000\n",
      "Epoch 55, loss : 0.0876, val_loss : 0.0601, acc : 0.938\n",
      "Epoch 56, loss : 0.0790, val_loss : 0.0379, acc : 1.000\n",
      "Epoch 57, loss : 0.0872, val_loss : 0.0944, acc : 0.938\n",
      "Epoch 58, loss : 0.0723, val_loss : 0.0873, acc : 0.938\n",
      "Epoch 59, loss : 0.0938, val_loss : 0.1745, acc : 0.938\n",
      "Epoch 60, loss : 0.0694, val_loss : 0.1656, acc : 0.938\n",
      "Epoch 61, loss : 0.1048, val_loss : 0.2726, acc : 0.938\n",
      "Epoch 62, loss : 0.0712, val_loss : 0.1686, acc : 0.938\n",
      "Epoch 63, loss : 0.1072, val_loss : 0.3124, acc : 0.938\n",
      "Epoch 64, loss : 0.0739, val_loss : 0.1537, acc : 0.938\n",
      "Epoch 65, loss : 0.1069, val_loss : 0.3194, acc : 0.938\n",
      "Epoch 66, loss : 0.0757, val_loss : 0.1524, acc : 0.938\n",
      "Epoch 67, loss : 0.1083, val_loss : 0.3198, acc : 0.938\n",
      "Epoch 68, loss : 0.0780, val_loss : 0.1584, acc : 0.938\n",
      "Epoch 69, loss : 0.1114, val_loss : 0.3150, acc : 0.938\n",
      "Epoch 70, loss : 0.0808, val_loss : 0.1598, acc : 0.938\n",
      "Epoch 71, loss : 0.1139, val_loss : 0.2882, acc : 0.938\n",
      "Epoch 72, loss : 0.0824, val_loss : 0.1420, acc : 0.938\n",
      "Epoch 73, loss : 0.1109, val_loss : 0.2295, acc : 0.938\n",
      "Epoch 74, loss : 0.0790, val_loss : 0.1058, acc : 0.938\n",
      "Epoch 75, loss : 0.1001, val_loss : 0.1764, acc : 0.938\n",
      "Epoch 76, loss : 0.0718, val_loss : 0.0576, acc : 0.938\n",
      "Epoch 77, loss : 0.0809, val_loss : 0.0943, acc : 0.938\n",
      "Epoch 78, loss : 0.0610, val_loss : 0.0220, acc : 1.000\n",
      "Epoch 79, loss : 0.0600, val_loss : 0.0219, acc : 1.000\n",
      "Epoch 80, loss : 0.0514, val_loss : 0.0118, acc : 1.000\n",
      "Epoch 81, loss : 0.0498, val_loss : 0.0060, acc : 1.000\n",
      "Epoch 82, loss : 0.0454, val_loss : 0.0173, acc : 1.000\n",
      "Epoch 83, loss : 0.0472, val_loss : 0.0077, acc : 1.000\n",
      "Epoch 84, loss : 0.0430, val_loss : 0.0290, acc : 1.000\n",
      "Epoch 85, loss : 0.0449, val_loss : 0.0094, acc : 1.000\n",
      "Epoch 86, loss : 0.0400, val_loss : 0.0495, acc : 0.938\n",
      "Epoch 87, loss : 0.0430, val_loss : 0.0088, acc : 1.000\n",
      "Epoch 88, loss : 0.0374, val_loss : 0.0671, acc : 0.938\n",
      "Epoch 89, loss : 0.0429, val_loss : 0.0068, acc : 1.000\n",
      "Epoch 90, loss : 0.0361, val_loss : 0.0711, acc : 0.938\n",
      "Epoch 91, loss : 0.0422, val_loss : 0.0058, acc : 1.000\n",
      "Epoch 92, loss : 0.0349, val_loss : 0.0694, acc : 0.938\n",
      "Epoch 93, loss : 0.0411, val_loss : 0.0054, acc : 1.000\n",
      "Epoch 94, loss : 0.0337, val_loss : 0.0679, acc : 0.938\n",
      "Epoch 95, loss : 0.0397, val_loss : 0.0059, acc : 1.000\n",
      "Epoch 96, loss : 0.0326, val_loss : 0.0614, acc : 0.938\n",
      "Epoch 97, loss : 0.0376, val_loss : 0.0080, acc : 1.000\n",
      "Epoch 98, loss : 0.0316, val_loss : 0.0500, acc : 0.938\n",
      "Epoch 99, loss : 0.0349, val_loss : 0.0137, acc : 1.000\n",
      "test_acc : 0.900\n"
     ]
    }
   ],
   "source": [
    "# 問題2\n",
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name() \n",
    "\n",
    "# データセットの読み込み\n",
    "df = pd.read_csv(\"data/Iris.csv\")\n",
    "\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "\n",
    "# NumPy 配列に変換\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "# ラベルを数値に変換\n",
    "y[y == \"Iris-versicolor\"] = 0\n",
    "y[y == \"Iris-virginica\"] = 1\n",
    "y = y.astype(np.int64)[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# eager_execution error回避\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.compat.v1.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.compat.v1.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    tf.random.set_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random.normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random.normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random.normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random.normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random.normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random.normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-nickname",
   "metadata": {},
   "source": [
    "#### 問題2\n",
    "scrach実装とtensorFlowとの対比\n",
    "\n",
    "- 重み、バイアスの初期化: example_net内の定義\n",
    "- 重み、バイアスの勾配計算、更新: train_op内の実装 `sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})`\n",
    "- 層の定義(何層用意するか、パラメータ設定): example_netの定義\n",
    "- 損失関数計算: `loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))`\n",
    "- 正解率計算: `accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))`\n",
    "- 学習率設定, Adagard実装: train_op内の実装\n",
    "- 入力Xに対する予測: ??sklearnでいうpredictはなし\n",
    "- 試行回数(epoch)のloop処理: session内のepochのloop\n",
    "- バッチで入力を分割して学習: scrachと同じ `for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):`\n",
    "- 活性関数定義: example_net内で各layerに定義\n",
    "- フォワードプロパゲージョン、バックプロパゲージョンの実装: `sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})`内の処理\n",
    "- 学習過程をdebug出力: epoch loopの最後"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wrapped-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの読み込み\n",
    "df = pd.read_csv(\"data/Iris.csv\")\n",
    "\n",
    "# データフレームから条件抽出\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "\n",
    "# NumPy 配列に変換\n",
    "X = np.array(X)\n",
    "# y = np.array(y)\n",
    "y = np.array(pd.get_dummies(df[\"Species\"]))\n",
    "# ラベルを数値に変換\n",
    "# y[y == \"Iris-setosa\"] = 0\n",
    "# y[y == \"Iris-virginica\"] = 1\n",
    "# y[y == \"Iris-versicolor\"] = 2\n",
    "# y = y.astype(np.int64)[:, np.newaxis]\n",
    "# y = y[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "selected-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "# eager_execution error回避\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.compat.v1.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.compat.v1.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "def Iris3_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    tf.random.set_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random.normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random.normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random.normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random.normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random.normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random.normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = Iris3_net(X)\n",
    "\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.argmax(Y, axis=1), tf.argmax(logits, axis=1))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.compat.v1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "challenging-likelihood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 11.3028, val_loss : 102.2221, acc : 0.292\n",
      "Epoch 1, loss : 8.7998, val_loss : 81.4813, acc : 0.375\n",
      "Epoch 2, loss : 6.8027, val_loss : 59.8930, acc : 0.542\n",
      "Epoch 3, loss : 4.7791, val_loss : 39.9069, acc : 0.542\n",
      "Epoch 4, loss : 3.0486, val_loss : 25.9002, acc : 0.208\n",
      "Epoch 5, loss : 1.6053, val_loss : 10.0990, acc : 0.458\n",
      "Epoch 6, loss : 0.4247, val_loss : 2.8406, acc : 0.708\n",
      "Epoch 7, loss : 0.0561, val_loss : 1.1800, acc : 0.792\n",
      "Epoch 8, loss : 0.0306, val_loss : 1.0144, acc : 0.875\n",
      "Epoch 9, loss : 0.0183, val_loss : 0.7311, acc : 0.917\n",
      "Epoch 10, loss : 0.0175, val_loss : 0.8112, acc : 0.917\n",
      "Epoch 11, loss : 0.0155, val_loss : 0.7299, acc : 0.917\n",
      "Epoch 12, loss : 0.0162, val_loss : 0.8065, acc : 0.917\n",
      "Epoch 13, loss : 0.0145, val_loss : 0.7146, acc : 0.917\n",
      "Epoch 14, loss : 0.0157, val_loss : 0.8388, acc : 0.917\n",
      "Epoch 15, loss : 0.0131, val_loss : 0.6572, acc : 0.917\n",
      "Epoch 16, loss : 0.0157, val_loss : 0.8889, acc : 0.917\n",
      "Epoch 17, loss : 0.0118, val_loss : 0.6051, acc : 0.833\n",
      "Epoch 18, loss : 0.0153, val_loss : 0.8688, acc : 0.917\n",
      "Epoch 19, loss : 0.0116, val_loss : 0.6202, acc : 0.917\n",
      "Epoch 20, loss : 0.0143, val_loss : 0.9097, acc : 0.917\n",
      "Epoch 21, loss : 0.0105, val_loss : 0.5645, acc : 0.833\n",
      "Epoch 22, loss : 0.0137, val_loss : 0.8264, acc : 0.917\n",
      "Epoch 23, loss : 0.0107, val_loss : 0.6523, acc : 0.917\n",
      "Epoch 24, loss : 0.0117, val_loss : 0.8234, acc : 0.917\n",
      "Epoch 25, loss : 0.0095, val_loss : 0.5578, acc : 0.875\n",
      "Epoch 26, loss : 0.0118, val_loss : 0.8526, acc : 0.917\n",
      "Epoch 27, loss : 0.0094, val_loss : 0.5413, acc : 0.875\n",
      "Epoch 28, loss : 0.0110, val_loss : 0.8185, acc : 0.917\n",
      "Epoch 29, loss : 0.0090, val_loss : 0.5916, acc : 0.917\n",
      "Epoch 30, loss : 0.0101, val_loss : 0.8104, acc : 0.917\n",
      "Epoch 31, loss : 0.0086, val_loss : 0.5880, acc : 0.917\n",
      "Epoch 32, loss : 0.0094, val_loss : 0.7882, acc : 0.917\n",
      "Epoch 33, loss : 0.0083, val_loss : 0.6213, acc : 0.917\n",
      "Epoch 34, loss : 0.0084, val_loss : 0.7378, acc : 0.917\n",
      "Epoch 35, loss : 0.0079, val_loss : 0.6718, acc : 0.917\n",
      "Epoch 36, loss : 0.0076, val_loss : 0.6888, acc : 0.917\n",
      "Epoch 37, loss : 0.0073, val_loss : 0.6835, acc : 0.917\n",
      "Epoch 38, loss : 0.0071, val_loss : 0.6921, acc : 0.917\n",
      "Epoch 39, loss : 0.0071, val_loss : 0.7064, acc : 0.917\n",
      "Epoch 40, loss : 0.0070, val_loss : 0.6946, acc : 0.917\n",
      "Epoch 41, loss : 0.0069, val_loss : 0.6993, acc : 0.917\n",
      "Epoch 42, loss : 0.0067, val_loss : 0.6906, acc : 0.917\n",
      "Epoch 43, loss : 0.0066, val_loss : 0.6927, acc : 0.917\n",
      "Epoch 44, loss : 0.0065, val_loss : 0.6860, acc : 0.917\n",
      "Epoch 45, loss : 0.0064, val_loss : 0.6826, acc : 0.917\n",
      "Epoch 46, loss : 0.0062, val_loss : 0.6764, acc : 0.917\n",
      "Epoch 47, loss : 0.0061, val_loss : 0.6715, acc : 0.917\n",
      "Epoch 48, loss : 0.0059, val_loss : 0.6650, acc : 0.917\n",
      "Epoch 49, loss : 0.0058, val_loss : 0.6595, acc : 0.917\n",
      "Epoch 50, loss : 0.0056, val_loss : 0.6535, acc : 0.917\n",
      "Epoch 51, loss : 0.0054, val_loss : 0.6492, acc : 0.917\n",
      "Epoch 52, loss : 0.0053, val_loss : 0.6432, acc : 0.917\n",
      "Epoch 53, loss : 0.0051, val_loss : 0.6389, acc : 0.917\n",
      "Epoch 54, loss : 0.0049, val_loss : 0.6329, acc : 0.917\n",
      "Epoch 55, loss : 0.0048, val_loss : 0.6285, acc : 0.917\n",
      "Epoch 56, loss : 0.0046, val_loss : 0.6230, acc : 0.917\n",
      "Epoch 57, loss : 0.0044, val_loss : 0.6183, acc : 0.917\n",
      "Epoch 58, loss : 0.0043, val_loss : 0.6133, acc : 0.917\n",
      "Epoch 59, loss : 0.0041, val_loss : 0.6086, acc : 0.917\n",
      "Epoch 60, loss : 0.0039, val_loss : 0.6040, acc : 0.917\n",
      "Epoch 61, loss : 0.0038, val_loss : 0.5994, acc : 0.917\n",
      "Epoch 62, loss : 0.0036, val_loss : 0.5950, acc : 0.917\n",
      "Epoch 63, loss : 0.0035, val_loss : 0.5907, acc : 0.917\n",
      "Epoch 64, loss : 0.0034, val_loss : 0.5829, acc : 0.917\n",
      "Epoch 65, loss : 0.0033, val_loss : 0.5940, acc : 0.917\n",
      "Epoch 66, loss : 0.0033, val_loss : 0.5821, acc : 0.917\n",
      "Epoch 67, loss : 0.0031, val_loss : 0.5734, acc : 0.917\n",
      "Epoch 68, loss : 0.0030, val_loss : 0.5729, acc : 0.917\n",
      "Epoch 69, loss : 0.0030, val_loss : 0.5712, acc : 0.917\n",
      "Epoch 70, loss : 0.0029, val_loss : 0.5684, acc : 0.917\n",
      "Epoch 71, loss : 0.0028, val_loss : 0.5691, acc : 0.917\n",
      "Epoch 72, loss : 0.0028, val_loss : 0.5659, acc : 0.917\n",
      "Epoch 73, loss : 0.0027, val_loss : 0.5624, acc : 0.917\n",
      "Epoch 74, loss : 0.0027, val_loss : 0.5607, acc : 0.917\n",
      "Epoch 75, loss : 0.0027, val_loss : 0.5690, acc : 0.917\n",
      "Epoch 76, loss : 0.0027, val_loss : 0.5603, acc : 0.917\n",
      "Epoch 77, loss : 0.0025, val_loss : 0.5527, acc : 0.917\n",
      "Epoch 78, loss : 0.0025, val_loss : 0.5537, acc : 0.917\n",
      "Epoch 79, loss : 0.0024, val_loss : 0.5545, acc : 0.917\n",
      "Epoch 80, loss : 0.0024, val_loss : 0.5537, acc : 0.917\n",
      "Epoch 81, loss : 0.0024, val_loss : 0.5498, acc : 0.917\n",
      "Epoch 82, loss : 0.0024, val_loss : 0.5539, acc : 0.917\n",
      "Epoch 83, loss : 0.0024, val_loss : 0.5540, acc : 0.917\n",
      "Epoch 84, loss : 0.0023, val_loss : 0.5488, acc : 0.917\n",
      "Epoch 85, loss : 0.0023, val_loss : 0.5460, acc : 0.917\n",
      "Epoch 86, loss : 0.0022, val_loss : 0.5448, acc : 0.917\n",
      "Epoch 87, loss : 0.0022, val_loss : 0.5456, acc : 0.917\n",
      "Epoch 88, loss : 0.0022, val_loss : 0.5465, acc : 0.917\n",
      "Epoch 89, loss : 0.0022, val_loss : 0.5459, acc : 0.917\n",
      "Epoch 90, loss : 0.0021, val_loss : 0.5448, acc : 0.917\n",
      "Epoch 91, loss : 0.0021, val_loss : 0.5430, acc : 0.917\n",
      "Epoch 92, loss : 0.0021, val_loss : 0.5427, acc : 0.917\n",
      "Epoch 93, loss : 0.0020, val_loss : 0.5434, acc : 0.917\n",
      "Epoch 94, loss : 0.0020, val_loss : 0.5436, acc : 0.917\n",
      "Epoch 95, loss : 0.0020, val_loss : 0.5429, acc : 0.917\n",
      "Epoch 96, loss : 0.0020, val_loss : 0.5419, acc : 0.917\n",
      "Epoch 97, loss : 0.0020, val_loss : 0.5412, acc : 0.917\n",
      "Epoch 98, loss : 0.0019, val_loss : 0.5408, acc : 0.917\n",
      "Epoch 99, loss : 0.0019, val_loss : 0.5407, acc : 0.917\n",
      "test_acc : 1.000\n"
     ]
    }
   ],
   "source": [
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "# 計算グラフの実行\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "raising-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題4\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# データセットの読み込み\n",
    "df = pd.read_csv(\"data/housing/train.csv\")\n",
    "\n",
    "# データフレームから条件抽出\n",
    "y = df[\"SalePrice\"]\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
    "\n",
    "# NumPy 配列に変換\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "# 標準化\n",
    "standardScaler_y = StandardScaler()\n",
    "standardScaler_X = StandardScaler()\n",
    "y = standardScaler_y.fit_transform(y)\n",
    "X = standardScaler_X.fit_transform(X)\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "studied-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# eager_execution error回避\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.compat.v1.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.compat.v1.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "def housingPrice_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    tf.random.set_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random.normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random.normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random.normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random.normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random.normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random.normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = housingPrice_net(X)\n",
    "\n",
    "# 目的関数\n",
    "loss_op = tf.compat.v1.losses.mean_squared_error(labels=Y, predictions=logits)\n",
    "# 最適化手法\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.compat.v1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vertical-characteristic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 24.5132, val_loss : 59.0078\n",
      "Epoch 1, loss : 4.0381, val_loss : 15.6652\n",
      "Epoch 2, loss : 1.5786, val_loss : 7.6257\n",
      "Epoch 3, loss : 0.8941, val_loss : 5.2186\n",
      "Epoch 4, loss : 0.6093, val_loss : 4.2106\n",
      "Epoch 5, loss : 0.4659, val_loss : 3.6830\n",
      "Epoch 6, loss : 0.3809, val_loss : 3.3319\n",
      "Epoch 7, loss : 0.3218, val_loss : 3.0504\n",
      "Epoch 8, loss : 0.2782, val_loss : 2.8167\n",
      "Epoch 9, loss : 0.2433, val_loss : 2.5851\n",
      "Epoch 10, loss : 0.2154, val_loss : 2.3353\n",
      "Epoch 11, loss : 0.1892, val_loss : 2.0991\n",
      "Epoch 12, loss : 0.1694, val_loss : 1.9041\n",
      "Epoch 13, loss : 0.1545, val_loss : 1.7625\n",
      "Epoch 14, loss : 0.1426, val_loss : 1.6403\n",
      "Epoch 15, loss : 0.1336, val_loss : 1.5269\n",
      "Epoch 16, loss : 0.1250, val_loss : 1.4517\n",
      "Epoch 17, loss : 0.1185, val_loss : 1.3770\n",
      "Epoch 18, loss : 0.1127, val_loss : 1.3043\n",
      "Epoch 19, loss : 0.1075, val_loss : 1.2504\n",
      "Epoch 20, loss : 0.1030, val_loss : 1.2006\n",
      "Epoch 21, loss : 0.0988, val_loss : 1.1451\n",
      "Epoch 22, loss : 0.0952, val_loss : 1.0918\n",
      "Epoch 23, loss : 0.0922, val_loss : 1.0629\n",
      "Epoch 24, loss : 0.0887, val_loss : 1.0272\n",
      "Epoch 25, loss : 0.0858, val_loss : 1.0050\n",
      "Epoch 26, loss : 0.0825, val_loss : 0.9898\n",
      "Epoch 27, loss : 0.0795, val_loss : 0.9829\n",
      "Epoch 28, loss : 0.0769, val_loss : 0.9758\n",
      "Epoch 29, loss : 0.0757, val_loss : 0.9899\n",
      "Epoch 30, loss : 0.0746, val_loss : 1.0130\n",
      "Epoch 31, loss : 0.0732, val_loss : 1.0057\n",
      "Epoch 32, loss : 0.0730, val_loss : 1.0367\n",
      "Epoch 33, loss : 0.0727, val_loss : 1.0337\n",
      "Epoch 34, loss : 0.0720, val_loss : 1.0458\n",
      "Epoch 35, loss : 0.0728, val_loss : 1.0536\n",
      "Epoch 36, loss : 0.0723, val_loss : 1.0715\n",
      "Epoch 37, loss : 0.0724, val_loss : 1.1038\n",
      "Epoch 38, loss : 0.0725, val_loss : 1.1148\n",
      "Epoch 39, loss : 0.0734, val_loss : 1.1530\n",
      "Epoch 40, loss : 0.0749, val_loss : 1.1558\n",
      "Epoch 41, loss : 0.0735, val_loss : 1.1557\n",
      "Epoch 42, loss : 0.0754, val_loss : 1.2198\n",
      "Epoch 43, loss : 0.0740, val_loss : 1.1766\n",
      "Epoch 44, loss : 0.0717, val_loss : 1.1736\n",
      "Epoch 45, loss : 0.0719, val_loss : 1.2048\n",
      "Epoch 46, loss : 0.0711, val_loss : 1.1338\n",
      "Epoch 47, loss : 0.0699, val_loss : 1.1401\n",
      "Epoch 48, loss : 0.0704, val_loss : 1.1081\n",
      "Epoch 49, loss : 0.0679, val_loss : 1.1594\n",
      "Epoch 50, loss : 0.0687, val_loss : 1.1337\n",
      "Epoch 51, loss : 0.0661, val_loss : 1.0773\n",
      "Epoch 52, loss : 0.0649, val_loss : 1.1360\n",
      "Epoch 53, loss : 0.0641, val_loss : 1.0387\n",
      "Epoch 54, loss : 0.0613, val_loss : 1.0223\n",
      "Epoch 55, loss : 0.0612, val_loss : 1.0391\n",
      "Epoch 56, loss : 0.0597, val_loss : 0.9753\n",
      "Epoch 57, loss : 0.0591, val_loss : 0.9846\n",
      "Epoch 58, loss : 0.0579, val_loss : 0.9299\n",
      "Epoch 59, loss : 0.0595, val_loss : 0.9527\n",
      "Epoch 60, loss : 0.0572, val_loss : 0.9152\n",
      "Epoch 61, loss : 0.0578, val_loss : 0.9025\n",
      "Epoch 62, loss : 0.0584, val_loss : 0.9307\n",
      "Epoch 63, loss : 0.0591, val_loss : 0.8970\n",
      "Epoch 64, loss : 0.0562, val_loss : 0.8763\n",
      "Epoch 65, loss : 0.0559, val_loss : 0.8606\n",
      "Epoch 66, loss : 0.0544, val_loss : 0.8423\n",
      "Epoch 67, loss : 0.0548, val_loss : 0.8229\n",
      "Epoch 68, loss : 0.0542, val_loss : 0.8144\n",
      "Epoch 69, loss : 0.0539, val_loss : 0.8222\n",
      "Epoch 70, loss : 0.0534, val_loss : 0.8108\n",
      "Epoch 71, loss : 0.0506, val_loss : 0.7762\n",
      "Epoch 72, loss : 0.0486, val_loss : 0.7646\n",
      "Epoch 73, loss : 0.0455, val_loss : 0.7468\n",
      "Epoch 74, loss : 0.0456, val_loss : 0.7391\n",
      "Epoch 75, loss : 0.0444, val_loss : 0.7372\n",
      "Epoch 76, loss : 0.0458, val_loss : 0.7439\n",
      "Epoch 77, loss : 0.0456, val_loss : 0.7228\n",
      "Epoch 78, loss : 0.0443, val_loss : 0.7030\n",
      "Epoch 79, loss : 0.0443, val_loss : 0.7127\n",
      "Epoch 80, loss : 0.0488, val_loss : 0.8030\n",
      "Epoch 81, loss : 0.0567, val_loss : 0.9713\n",
      "Epoch 82, loss : 0.0722, val_loss : 1.0000\n",
      "Epoch 83, loss : 0.0470, val_loss : 0.7086\n",
      "Epoch 84, loss : 0.0477, val_loss : 0.6472\n",
      "Epoch 85, loss : 0.0451, val_loss : 0.8260\n",
      "Epoch 86, loss : 0.0530, val_loss : 1.0384\n",
      "Epoch 87, loss : 0.0650, val_loss : 0.7799\n",
      "Epoch 88, loss : 0.0499, val_loss : 0.6820\n",
      "Epoch 89, loss : 0.0654, val_loss : 0.7257\n",
      "Epoch 90, loss : 0.0522, val_loss : 0.8169\n",
      "Epoch 91, loss : 0.0702, val_loss : 0.7988\n",
      "Epoch 92, loss : 0.0409, val_loss : 0.6441\n",
      "Epoch 93, loss : 0.0407, val_loss : 0.6285\n",
      "Epoch 94, loss : 0.0489, val_loss : 0.6903\n",
      "Epoch 95, loss : 0.0593, val_loss : 0.9085\n",
      "Epoch 96, loss : 0.0602, val_loss : 0.8920\n",
      "Epoch 97, loss : 0.0490, val_loss : 0.9093\n",
      "Epoch 98, loss : 0.0404, val_loss : 0.7757\n",
      "Epoch 99, loss : 0.0395, val_loss : 0.6756\n",
      "loss_op : 0.997\n"
     ]
    }
   ],
   "source": [
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "pred_price = None\n",
    "# 計算グラフの実行\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, total_loss, val_loss))\n",
    "        pred_price = logits\n",
    "    test_loss = sess.run(loss_op, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"loss_op : {:.3f}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "enhanced-boating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_8:0' shape=(None, 1) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO 実際の予測値が見たいがtf.Tensorをnumpyで出力する方法がわからない。 .numpy()はエラー\n",
    "pred_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "based-spine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (60000, 28, 28)\n",
      "X_test:  (10000, 28, 28)\n",
      "X_train reshaped:  (60000, 784)\n",
      "X_test reshaped:  (10000, 784)\n",
      "y_train:  (60000, 10)\n",
      "y_test:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 問題5\n",
    "# データセットの読み込み\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "# データフレームから条件抽出\n",
    "\n",
    "# 0 1スケールに変更\n",
    "X_train = X_train / 255\n",
    "\n",
    "# 1次元に変換\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1] ** 2))\n",
    "print(\"X_train reshaped: \", X_train.shape)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1] ** 2))\n",
    "print(\"X_test reshaped: \", X_test.shape)\n",
    "# y次元変換\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "y_train = y_train[:, np.newaxis]\n",
    "y_test = y_test[:, np.newaxis]\n",
    "y_train = ohe.fit_transform(y_train).toarray()\n",
    "y_test = ohe.transform(y_test).toarray()\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)\n",
    "\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "neither-church",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "varied-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10\n",
    "\n",
    "# eager_execution error回避\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.compat.v1.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.compat.v1.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "def mnist_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    tf.random.set_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random.normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random.normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random.normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random.normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random.normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random.normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = mnist_net(X)\n",
    "\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.argmax(Y, axis=1), tf.argmax(logits, axis=1))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.compat.v1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "statewide-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 2.7981, val_loss : 8.1544, acc : 0.799\n",
      "Epoch 1, loss : 0.4917, val_loss : 3.6453, acc : 0.830\n",
      "Epoch 2, loss : 0.2260, val_loss : 1.9663, acc : 0.840\n",
      "Epoch 3, loss : 0.1227, val_loss : 1.2979, acc : 0.848\n",
      "Epoch 4, loss : 0.0785, val_loss : 1.0034, acc : 0.863\n",
      "Epoch 5, loss : 0.0585, val_loss : 0.8404, acc : 0.873\n",
      "Epoch 6, loss : 0.0466, val_loss : 0.7679, acc : 0.884\n",
      "Epoch 7, loss : 0.0392, val_loss : 0.7292, acc : 0.889\n",
      "Epoch 8, loss : 0.0337, val_loss : 0.6963, acc : 0.894\n",
      "Epoch 9, loss : 0.0299, val_loss : 0.6603, acc : 0.902\n",
      "Epoch 10, loss : 0.0269, val_loss : 0.6562, acc : 0.902\n",
      "Epoch 11, loss : 0.0245, val_loss : 0.6385, acc : 0.907\n",
      "Epoch 12, loss : 0.0229, val_loss : 0.5725, acc : 0.910\n",
      "Epoch 13, loss : 0.0210, val_loss : 0.6171, acc : 0.911\n",
      "Epoch 14, loss : 0.0196, val_loss : 0.5930, acc : 0.918\n",
      "Epoch 15, loss : 0.0184, val_loss : 0.5959, acc : 0.919\n",
      "Epoch 16, loss : 0.0173, val_loss : 0.6007, acc : 0.917\n",
      "Epoch 17, loss : 0.0161, val_loss : 0.5915, acc : 0.918\n",
      "Epoch 18, loss : 0.0156, val_loss : 0.5888, acc : 0.920\n",
      "Epoch 19, loss : 0.0148, val_loss : 0.5951, acc : 0.921\n",
      "Epoch 20, loss : 0.0138, val_loss : 0.5708, acc : 0.925\n",
      "Epoch 21, loss : 0.0131, val_loss : 0.5787, acc : 0.927\n",
      "Epoch 22, loss : 0.0128, val_loss : 0.6111, acc : 0.921\n",
      "Epoch 23, loss : 0.0127, val_loss : 0.6195, acc : 0.926\n",
      "Epoch 24, loss : 0.0116, val_loss : 0.5962, acc : 0.928\n",
      "Epoch 25, loss : 0.0115, val_loss : 0.6249, acc : 0.928\n",
      "Epoch 26, loss : 0.0112, val_loss : 0.6006, acc : 0.931\n",
      "Epoch 27, loss : 0.0106, val_loss : 0.6146, acc : 0.931\n",
      "Epoch 28, loss : 0.0101, val_loss : 0.5985, acc : 0.932\n",
      "Epoch 29, loss : 0.0099, val_loss : 0.6041, acc : 0.931\n",
      "Epoch 30, loss : 0.0097, val_loss : 0.6161, acc : 0.931\n",
      "Epoch 31, loss : 0.0095, val_loss : 0.6612, acc : 0.931\n",
      "Epoch 32, loss : 0.0091, val_loss : 0.6430, acc : 0.933\n",
      "Epoch 33, loss : 0.0089, val_loss : 0.6429, acc : 0.933\n",
      "Epoch 34, loss : 0.0084, val_loss : 0.6137, acc : 0.938\n",
      "Epoch 35, loss : 0.0081, val_loss : 0.6199, acc : 0.935\n",
      "Epoch 36, loss : 0.0079, val_loss : 0.6226, acc : 0.935\n",
      "Epoch 37, loss : 0.0077, val_loss : 0.6312, acc : 0.936\n",
      "Epoch 38, loss : 0.0074, val_loss : 0.6384, acc : 0.937\n",
      "Epoch 39, loss : 0.0071, val_loss : 0.6638, acc : 0.935\n",
      "Epoch 40, loss : 0.0071, val_loss : 0.6428, acc : 0.937\n",
      "Epoch 41, loss : 0.0067, val_loss : 0.6557, acc : 0.941\n",
      "Epoch 42, loss : 0.0066, val_loss : 0.6565, acc : 0.938\n",
      "Epoch 43, loss : 0.0064, val_loss : 0.6702, acc : 0.939\n",
      "Epoch 44, loss : 0.0065, val_loss : 0.6801, acc : 0.938\n",
      "Epoch 45, loss : 0.0060, val_loss : 0.6946, acc : 0.936\n",
      "Epoch 46, loss : 0.0059, val_loss : 0.6551, acc : 0.939\n",
      "Epoch 47, loss : 0.0059, val_loss : 0.6770, acc : 0.939\n",
      "Epoch 48, loss : 0.0056, val_loss : 0.6864, acc : 0.941\n",
      "Epoch 49, loss : 0.0056, val_loss : 0.6778, acc : 0.940\n",
      "Epoch 50, loss : 0.0053, val_loss : 0.6948, acc : 0.940\n",
      "Epoch 51, loss : 0.0055, val_loss : 0.7192, acc : 0.941\n",
      "Epoch 52, loss : 0.0051, val_loss : 0.7269, acc : 0.941\n",
      "Epoch 53, loss : 0.0052, val_loss : 0.7081, acc : 0.939\n",
      "Epoch 54, loss : 0.0051, val_loss : 0.7078, acc : 0.941\n",
      "Epoch 55, loss : 0.0050, val_loss : 0.7242, acc : 0.939\n",
      "Epoch 56, loss : 0.0047, val_loss : 0.7559, acc : 0.939\n",
      "Epoch 57, loss : 0.0048, val_loss : 0.7231, acc : 0.938\n",
      "Epoch 58, loss : 0.0048, val_loss : 0.7402, acc : 0.941\n",
      "Epoch 59, loss : 0.0045, val_loss : 0.7415, acc : 0.941\n",
      "Epoch 60, loss : 0.0045, val_loss : 0.7273, acc : 0.942\n",
      "Epoch 61, loss : 0.0043, val_loss : 0.7208, acc : 0.942\n",
      "Epoch 62, loss : 0.0042, val_loss : 0.7313, acc : 0.939\n",
      "Epoch 63, loss : 0.0044, val_loss : 0.7405, acc : 0.943\n",
      "Epoch 64, loss : 0.0042, val_loss : 0.7609, acc : 0.941\n",
      "Epoch 65, loss : 0.0043, val_loss : 0.7654, acc : 0.941\n",
      "Epoch 66, loss : 0.0041, val_loss : 0.7479, acc : 0.943\n",
      "Epoch 67, loss : 0.0039, val_loss : 0.7880, acc : 0.941\n",
      "Epoch 68, loss : 0.0041, val_loss : 0.7525, acc : 0.944\n",
      "Epoch 69, loss : 0.0038, val_loss : 0.7488, acc : 0.943\n",
      "Epoch 70, loss : 0.0038, val_loss : 0.7801, acc : 0.942\n",
      "Epoch 71, loss : 0.0036, val_loss : 0.7727, acc : 0.941\n",
      "Epoch 72, loss : 0.0039, val_loss : 0.7900, acc : 0.942\n",
      "Epoch 73, loss : 0.0036, val_loss : 0.7915, acc : 0.941\n",
      "Epoch 74, loss : 0.0035, val_loss : 0.7892, acc : 0.942\n",
      "Epoch 75, loss : 0.0033, val_loss : 0.8100, acc : 0.942\n",
      "Epoch 76, loss : 0.0035, val_loss : 0.8212, acc : 0.944\n",
      "Epoch 77, loss : 0.0033, val_loss : 0.8199, acc : 0.943\n",
      "Epoch 78, loss : 0.0034, val_loss : 0.8317, acc : 0.944\n",
      "Epoch 79, loss : 0.0033, val_loss : 0.8448, acc : 0.942\n",
      "Epoch 80, loss : 0.0035, val_loss : 0.8336, acc : 0.943\n",
      "Epoch 81, loss : 0.0032, val_loss : 0.8325, acc : 0.942\n",
      "Epoch 82, loss : 0.0032, val_loss : 0.8197, acc : 0.943\n",
      "Epoch 83, loss : 0.0031, val_loss : 0.8194, acc : 0.942\n",
      "Epoch 84, loss : 0.0032, val_loss : 0.8671, acc : 0.942\n",
      "Epoch 85, loss : 0.0031, val_loss : 0.8369, acc : 0.946\n",
      "Epoch 86, loss : 0.0028, val_loss : 0.8572, acc : 0.942\n",
      "Epoch 87, loss : 0.0030, val_loss : 0.8612, acc : 0.945\n",
      "Epoch 88, loss : 0.0029, val_loss : 0.8987, acc : 0.944\n",
      "Epoch 89, loss : 0.0029, val_loss : 0.9105, acc : 0.944\n",
      "Epoch 90, loss : 0.0029, val_loss : 0.8846, acc : 0.945\n",
      "Epoch 91, loss : 0.0029, val_loss : 0.8546, acc : 0.946\n",
      "Epoch 92, loss : 0.0029, val_loss : 0.8874, acc : 0.947\n",
      "Epoch 93, loss : 0.0029, val_loss : 0.8797, acc : 0.946\n",
      "Epoch 94, loss : 0.0027, val_loss : 0.8981, acc : 0.944\n",
      "Epoch 95, loss : 0.0025, val_loss : 0.9296, acc : 0.944\n",
      "Epoch 96, loss : 0.0028, val_loss : 0.9022, acc : 0.944\n",
      "Epoch 97, loss : 0.0024, val_loss : 0.9816, acc : 0.946\n",
      "Epoch 98, loss : 0.0026, val_loss : 0.9348, acc : 0.944\n",
      "Epoch 99, loss : 0.0023, val_loss : 1.0140, acc : 0.946\n",
      "test_acc : 0.934\n"
     ]
    }
   ],
   "source": [
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "# 計算グラフの実行\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
