{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "living-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from IPython.display import Image\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99f98b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "uint8\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAP/klEQVR4nO3dfaxUdX7H8fdH1LYiitQWKYuysBajxrIbxNaQVeOyKtHgVWuW1oQGIqYrjTYtqaV/rKbF2vrQSNxYrlEXmi26iRqQ7i5aULFrQ7wiKuKi1mCEXmENIg8+Ffj2jzm4V7zzm8vMmQfu7/NKJnfmfM+Z8/XED+fMnHPmp4jAzAa/o9rdgJm1hsNulgmH3SwTDrtZJhx2s0w47GaZcNiPcJI2S/rOAOcNSd+ocz11L2udwWG3ppP0rKRPJe0pHpva3VOOHHZrlbkRcXzxmNDuZnLksA8ikiZL+m9JOyX1SrpP0rGHzDZN0juSPpB0p6Sj+iw/S9Ibkj6UtFLSaS3+T7AmctgHl/3AXwInA38EXAx8/5B5uoBJwLeA6cAsAEnTgfnAVcDvAM8DSweyUkm3SFpRY7Z/LP6B+YWkCwfyvlayiPDjCH4Am4HvVKndDDzR53UAl/Z5/X1gVfH8Z8DsPrWjgI+B0/os+406ezwPGAb8BjAT2A2Mb/e2y+3hPfsgIun3Ja2Q9L6kXcDtVPbyfb3X5/m7wO8Vz08D7i0+AuwEdgACRjfaV0SsjYjdEfFZRCwGfgFMa/R97fA47IPL/cAvgdMj4gQqh+U6ZJ4xfZ6fCvxv8fw94IaIGN7n8VsR8UIT+ox++rImc9gHl2HALmCPpDOAP+9nnnmSTpI0BrgJeLSY/q/A30o6C0DSiZL+uNGGJA2XdImk35R0tKQ/Bb4N/LzR97bD47APLn8N/AmVz8QP8Osg97UMeAlYD/wH8CBARDwB/BPwSPERYANw2UBWKmm+pJ9VKR8D/APwK+AD4C+AKyPizYH9J1lZVHyBYmaDnPfsZplw2M0y4bCbZcJhN8vE0a1cmSR/G2jWZBHR7zUMDe3ZJV0qaZOktyXd0sh7mVlz1X3qTdIQ4E1gKrAFeBGYEREbE8t4z27WZM3Ys08G3o6IdyLic+ARKndRmVkHaiTso/nyTRVb6OemCUlzJPVI6mlgXWbWoKZ/QRcR3UA3+DDerJ0a2bNv5ct3UH2tmGZmHaiRsL8InC7p68VPH30PWF5OW2ZWtroP4yNin6S5wEpgCPBQRLxeWmdmVqqW3vXmz+xmzdeUi2rM7MjhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sE3UP2WxHhiFDhiTrJ554YlPXP3fu3Kq14447LrnshAkTkvUbb7wxWb/rrruq1mbMmJFc9tNPP03W77jjjmT9tttuS9bboaGwS9oM7Ab2A/siYlIZTZlZ+crYs18UER+U8D5m1kT+zG6WiUbDHsBTkl6SNKe/GSTNkdQjqafBdZlZAxo9jJ8SEVsl/S7wtKRfRsSavjNERDfQDSApGlyfmdWpoT17RGwt/m4HngAml9GUmZWv7rBLGipp2MHnwHeBDWU1ZmblauQwfiTwhKSD7/PvEfHzUroaZE499dRk/dhjj03Wzz///GR9ypQpVWvDhw9PLnv11Vcn6+20ZcuWZH3hwoXJeldXV9Xa7t27k8u+8soryfpzzz2XrHeiusMeEe8Af1BiL2bWRD71ZpYJh90sEw67WSYcdrNMOOxmmVBE6y5qG6xX0E2cODFZX716dbLe7NtMO9WBAweS9VmzZiXre/bsqXvdvb29yfqHH36YrG/atKnudTdbRKi/6d6zm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZ8Hn2EowYMSJZX7t2bbI+bty4MtspVa3ed+7cmaxfdNFFVWuff/55ctlcrz9olM+zm2XOYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZ8JDNJdixY0eyPm/evGT98ssvT9ZffvnlZL3WTyqnrF+/PlmfOnVqsr53795k/ayzzqpau+mmm5LLWrm8ZzfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuH72TvACSeckKzXGl540aJFVWuzZ89OLnvdddcl60uXLk3WrfPUfT+7pIckbZe0oc+0EZKelvRW8fekMps1s/IN5DD+R8Clh0y7BVgVEacDq4rXZtbBaoY9ItYAh14POh1YXDxfDFxZbltmVrZ6r40fGREHB8t6HxhZbUZJc4A5da7HzErS8I0wERGpL94iohvoBn9BZ9ZO9Z562yZpFEDxd3t5LZlZM9Qb9uXAzOL5TGBZOe2YWbPUPIyXtBS4EDhZ0hbgB8AdwE8kzQbeBa5tZpOD3a5duxpa/qOPPqp72euvvz5Zf/TRR5P1WmOsW+eoGfaImFGldHHJvZhZE/lyWbNMOOxmmXDYzTLhsJtlwmE3y4RvcR0Ehg4dWrX25JNPJpe94IILkvXLLrssWX/qqaeSdWs9D9lsljmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XC59kHufHjxyfr69atS9Z37tyZrD/zzDPJek9PT9XaD3/4w+Syrfx/czDxeXazzDnsZplw2M0y4bCbZcJhN8uEw26WCYfdLBM+z565rq6uZP3hhx9O1ocNG1b3uufPn5+sL1myJFnv7e1N1nPl8+xmmXPYzTLhsJtlwmE3y4TDbpYJh90sEw67WSZ8nt2Szj777GT9nnvuSdYvvrj+wX4XLVqUrC9YsCBZ37p1a93rPpLVfZ5d0kOStkva0GfarZK2SlpfPKaV2ayZlW8gh/E/Ai7tZ/q/RMTE4vHTctsys7LVDHtErAF2tKAXM2uiRr6gmyvp1eIw/6RqM0maI6lHUvUfIzOzpqs37PcD44GJQC9wd7UZI6I7IiZFxKQ612VmJagr7BGxLSL2R8QB4AFgcrltmVnZ6gq7pFF9XnYBG6rNa2adoeZ5dklLgQuBk4FtwA+K1xOBADYDN0REzZuLfZ598Bk+fHiyfsUVV1St1bpXXur3dPEXVq9enaxPnTo1WR+sqp1nP3oAC87oZ/KDDXdkZi3ly2XNMuGwm2XCYTfLhMNulgmH3SwTvsXV2uazzz5L1o8+On2yaN++fcn6JZdcUrX27LPPJpc9kvmnpM0y57CbZcJhN8uEw26WCYfdLBMOu1kmHHazTNS8683yds455yTr11xzTbJ+7rnnVq3VOo9ey8aNG5P1NWvWNPT+g4337GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJnyefZCbMGFCsj537txk/aqrrkrWTznllMPuaaD279+frPf2pn+9/MCBA2W2c8Tznt0sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y0TN8+ySxgBLgJFUhmjujoh7JY0AHgXGUhm2+dqI+LB5rear1rnsGTP6G2i3otZ59LFjx9bTUil6enqS9QULFiTry5cvL7OdQW8ge/Z9wF9FxJnAHwI3SjoTuAVYFRGnA6uK12bWoWqGPSJ6I2Jd8Xw38AYwGpgOLC5mWwxc2aQezawEh/WZXdJY4JvAWmBkRBy8XvF9Kof5ZtahBnxtvKTjgceAmyNil/Tr4aQiIqqN4yZpDjCn0UbNrDED2rNLOoZK0H8cEY8Xk7dJGlXURwHb+1s2IrojYlJETCqjYTOrT82wq7ILfxB4IyLu6VNaDswsns8ElpXfnpmVpeaQzZKmAM8DrwEH7xmcT+Vz+0+AU4F3qZx621HjvbIcsnnkyPTXGWeeeWayft999yXrZ5xxxmH3VJa1a9cm63feeWfV2rJl6f2Db1GtT7Uhm2t+Zo+I/wL6XRi4uJGmzKx1fAWdWSYcdrNMOOxmmXDYzTLhsJtlwmE3y4R/SnqARowYUbW2aNGi5LITJ05M1seNG1dPS6V44YUXkvW77747WV+5cmWy/sknnxx2T9Yc3rObZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZpnI5jz7eeedl6zPmzcvWZ88eXLV2ujRo+vqqSwff/xx1drChQuTy95+++3J+t69e+vqyTqP9+xmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSayOc/e1dXVUL0RGzduTNZXrFiRrO/bty9ZT91zvnPnzuSylg/v2c0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTAxkfPYxwBJgJBBAd0TcK+lW4HrgV8Ws8yPipzXeK8vx2c1aqdr47AMJ+yhgVESskzQMeAm4ErgW2BMRdw20CYfdrPmqhb3mFXQR0Qv0Fs93S3oDaO9Ps5jZYTusz+ySxgLfBNYWk+ZKelXSQ5JOqrLMHEk9knoaa9XMGlHzMP6LGaXjgeeABRHxuKSRwAdUPsf/PZVD/Vk13sOH8WZNVvdndgBJxwArgJURcU8/9bHAiog4u8b7OOxmTVYt7DUP4yUJeBB4o2/Qiy/uDuoCNjTapJk1z0C+jZ8CPA+8BhwoJs8HZgATqRzGbwZuKL7MS72X9+xmTdbQYXxZHHaz5qv7MN7MBgeH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMtHqIZs/AN7t8/rkYlon6tTeOrUvcG/1KrO306oVWno/+1dWLvVExKS2NZDQqb11al/g3urVqt58GG+WCYfdLBPtDnt3m9ef0qm9dWpf4N7q1ZLe2vqZ3cxap917djNrEYfdLBNtCbukSyVtkvS2pFva0UM1kjZLek3S+naPT1eMobdd0oY+00ZIelrSW8XffsfYa1Nvt0raWmy79ZKmtam3MZKekbRR0uuSbiqmt3XbJfpqyXZr+Wd2SUOAN4GpwBbgRWBGRGxsaSNVSNoMTIqItl+AIenbwB5gycGhtST9M7AjIu4o/qE8KSL+pkN6u5XDHMa7Sb1VG2b8z2jjtitz+PN6tGPPPhl4OyLeiYjPgUeA6W3oo+NFxBpgxyGTpwOLi+eLqfzP0nJVeusIEdEbEeuK57uBg8OMt3XbJfpqiXaEfTTwXp/XW+is8d4DeErSS5LmtLuZfozsM8zW+8DIdjbTj5rDeLfSIcOMd8y2q2f480b5C7qvmhIR3wIuA24sDlc7UlQ+g3XSudP7gfFUxgDsBe5uZzPFMOOPATdHxK6+tXZuu376asl2a0fYtwJj+rz+WjGtI0TE1uLvduAJKh87Osm2gyPoFn+3t7mfL0TEtojYHxEHgAdo47Yrhhl/DPhxRDxeTG77tuuvr1Ztt3aE/UXgdElfl3Qs8D1geRv6+ApJQ4svTpA0FPgunTcU9XJgZvF8JrCsjb18SacM411tmHHavO3aPvx5RLT8AUyj8o38/wB/144eqvQ1DnileLze7t6ApVQO6/6Pyncbs4HfBlYBbwH/CYzooN7+jcrQ3q9SCdaoNvU2hcoh+qvA+uIxrd3bLtFXS7abL5c1y4S/oDPLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMvH/+Oizgu2jpN0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MNISTデータをmodel入力用に整形\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape) # (60000, 28, 28)\n",
    "print(X_test.shape) # (10000, 28, 28)\n",
    "print(X_train[0].dtype) # uint8\n",
    "print(X_train[0])\n",
    "# 1次元にする\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# 一つ可視化\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "index = 0\n",
    "image = X_train[index].reshape(28,28)\n",
    "# X_train[index]: (784,)\n",
    "# image: (28, 28)\n",
    "plt.imshow(image, 'gray')\n",
    "plt.title('label : {}'.format(y_train[index]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unnecessary-violation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(48000,)\n",
      "(48000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "# MNISTデータをmodel入力用に整形\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 28, 28)\n",
    "X_test = X_test.reshape(-1, 28, 28)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# scaleを0 ~ 255から0 ~ 1にする\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# トレーニングとテストデータに分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "# OHE作成\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_val_one_hot = enc.transform(y_val[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "protected-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチ\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "peaceful-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果確認、分析用の関数\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# scoreまとめ表示\n",
    "def eval_accuracy(model, X_test, y_test, model_name):\n",
    "    pred_y = model.predict(X_test)\n",
    "    nn_result = pd.DataFrame(\n",
    "        [accuracy_score(y_test, pred_y), precision_score(y_test, pred_y, average='micro'), recall_score(y_test, pred_y, average='micro')],\n",
    "        index=['Accuracy', 'Precision', 'Recall'],\n",
    "        columns=[model_name]\n",
    "    )\n",
    "    return nn_result\n",
    "\n",
    "# 損失値の推移\n",
    "def display_loss_graph(model):\n",
    "    iter_list = list(range(len(model.loss_train)))\n",
    "    plt.scatter(iter_list, model.loss_train, label=\"train loss\")\n",
    "    plt.scatter(iter_list, model.loss_val, label=\"test loss\")\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iter')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91284478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return self.sigmoid(A)\n",
    "    def backward(self, dZ):\n",
    "        _sig = self.sigmoid(self.A)\n",
    "        return dZ * (1 - _sig)*_sig\n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "    def backward(self, dZ):\n",
    "        return dZ * (1 - (np.tanh(self.A))**2)\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, X):\n",
    "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
    "        return self.Z\n",
    "    def backward(self, Y):\n",
    "        self.loss = self.loss_func(Y)\n",
    "        return self.Z - Y\n",
    "    def loss_func(self, Y, Z=None):\n",
    "        if Z is None:\n",
    "            Z = self.Z\n",
    "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.clip(A, 0, None)\n",
    "    def backward(self, dZ):\n",
    "        return dZ * np.clip(np.sign(self.A), 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83cf2894",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [ReLU(),ReLU(),Tanh(),Softmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68f7e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, activation):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.A = X@self.W + self.B\n",
    "        return self.activation.forward(self.A)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "\n",
    "        dA = self.activation.backward(dZ)\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.dW = self.X.T@dA\n",
    "        dZ = dA@self.W.T\n",
    "\n",
    "        self.optimizer.update(self)\n",
    "\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35b62cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.sigma = math.sqrt(1 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B\n",
    "    \n",
    "class HeInitializer():\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.sigma = math.sqrt(2 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B\n",
    "        \n",
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, *shape):\n",
    "        W = self.sigma * np.random.randn(*shape)\n",
    "        return W\n",
    "    def B(self, *shape):\n",
    "        B = self.sigma * np.random.randn(*shape)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8c87207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        return\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.HW = 1\n",
    "        self.HB = 1\n",
    "    def update(self, layer):\n",
    "        self.HW += layer.dW**2\n",
    "        self.HB += layer.dB**2\n",
    "        layer.W -= self.lr * np.sqrt(1/self.HW) * layer.dW\n",
    "        layer.B -= self.lr * np.sqrt(1/self.HB) * layer.dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0944cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv2d:\n",
    "    # F: フィルタ数\n",
    "    # FH: フィルター高さ\n",
    "    # FW: フィルター幅\n",
    "    # P: パディング数\n",
    "    # S: ストライド数\n",
    "    # C: チャンネル数\n",
    "    def __init__(self, F, C, FH, FW, P, S, initializer=None, optimizer=None, activation=None):\n",
    "        self.P = P\n",
    "        self.S = S\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        self.W = self.initializer.W(F, C, FH, FW)\n",
    "        self.B = self.initializer.B(F)\n",
    "        \n",
    "    def forward(self, X,debug=False):\n",
    "        self.X = X\n",
    "        # N: レコード数\n",
    "        # C: チャネル数\n",
    "        # H: 高さ\n",
    "        # W: 幅\n",
    "        # F: フィルター数\n",
    "        N, C, H, W = self.X.shape\n",
    "        F, C, FH, FW = self.W.shape\n",
    "        OH, OW = self.output_shape2d(H, W, self.P, self.P, FH, FW, self.S, self.S)\n",
    "        self.params = N, C, H, W, F, FH, FW, OH, OW\n",
    "        A = np.zeros([N,F,OH,OW])\n",
    "        self.X_pad = np.pad(self.X,((0,0),(0,0),(self.P,self.P),(self.P,self.P)))\n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(0,H,self.S):\n",
    "                    for col in range(0,W,self.S):\n",
    "                        if self.P == 0 and (W-2 <= col or H-2<=row):\n",
    "                            continue\n",
    "                        A[n,ch,row,col] = np.sum(self.X_pad[n,:,row:row+FH,col:col+FW]*self.W[ch,:,:,:]) +self.B[ch]\n",
    "#         print(\"A: \", A.shape, A)\n",
    "        if debug==True:\n",
    "            return A\n",
    "        else:\n",
    "            return  self.activation.forward(A)\n",
    "\n",
    "    def backward(self, dZ,debug=False):\n",
    "        if debug==True:\n",
    "            dA = dZ\n",
    "        else:\n",
    "            dA = self.activation.backward(dZ)\n",
    "        N,C,H,W,F,FH,FW,OH,OW = self.params\n",
    "        dZ = np.zeros(self.X_pad.shape)\n",
    "        self.dW = np.zeros(self.W.shape)\n",
    "        self.dB = np.zeros(self.B.shape)\n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(0,H,self.S):\n",
    "                    for col in range(0,W,self.S):\n",
    "                        if self.P == 0 and (W-2 <= col or H-2<=row):\n",
    "                            continue\n",
    "                        dZ[n,:,row:row+FH,col:col+FW] += dA[n,ch,row,col]*self.W[ch,:,:,:]\n",
    "        if self.P == 0:\n",
    "            dZ = np.delete(dZ,[0,H-1],axis=2)\n",
    "            dZ = np.delete(dZ,[0,W-1],axis=3)\n",
    "        else:\n",
    "            dl_rows = range(self.P),range(H+self.P,H+2*self.P,1)\n",
    "            dl_cols = range(self.P),range(W+self.P,W+2*self.P,1)\n",
    "            dZ = np.delete(dZ,dl_rows,axis=2)\n",
    "            dZ = np.delete(dZ,dl_cols,axis=3)\n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(OH):\n",
    "                    for col in range(OW):\n",
    "                        self.dW[ch,:,:,:] += dA[n,ch,row,col]*self.X_pad[n,:,row:row+FH,col:col+FW]\n",
    "        for ch in range(F):\n",
    "            self.dB[ch] = np.sum(dA[:,ch,:,:])\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "    \n",
    "    def output_shape2d(self, H, W, PH, PW, FH, FW, SH, SW):\n",
    "        OH = (H + 2*PH - FH) / SH + 1\n",
    "        OW = (W + 2*PW - FW) / SW + 1\n",
    "\n",
    "        return int(OH), int(OW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "433fe217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializerConv2d:\n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, F, C, FH, FW):\n",
    "        return self.sigma * np.random.randn(F, C, FH, FW)\n",
    "\n",
    "    def B(self, F):\n",
    "        return np.zeros(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fc5a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[[ 1,  2,  3,  4],[ 5,  6,  7,  8],[ 9, 10, 11, 12],[13, 14, 15, 16]]]])\n",
    "w = np.array([[[[ 0.,  0.,  0.],[ 0.,  1.,  0.],[ 0., -1.,  0.]]],[[[ 0.,  0.,  0.],[ 0., -1.,  1.],[ 0.,  0.,  0.]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b7dd7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-4. -4.]\n",
      "   [-4. -4.]]\n",
      "\n",
      "  [[ 1.  1.]\n",
      "   [ 1.  1.]]]]\n"
     ]
    }
   ],
   "source": [
    "simple_conv_2d = SimpleConv2d(F=2, C=1, FH=3, FW=3, P=0, S=1,initializer=SimpleInitializerConv2d(),optimizer=SGD(0.01), activation=ReLU())\n",
    "simple_conv_2d.W = w\n",
    "A = simple_conv_2d.forward(x,True)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "277bd67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-5.  4.]\n",
      "   [13. 27.]]]]\n"
     ]
    }
   ],
   "source": [
    "da = np.array([[[[ -4,  -4], [ 10,  11]],[[  1,  -7],[  1, -11]]]])\n",
    "dZ = simple_conv_2d.backward(da,True)\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8922ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_conv_2d.output_shape2d(H=6,W=6,PH=0,PW=0,FH=3,FW=3,SH=1,SW=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b0ecb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D():\n",
    "    def __init__(self,P):\n",
    "        self.P = P\n",
    "        self.PA = None\n",
    "        self.Pindex = None\n",
    "\n",
    "    def forward(self,A):\n",
    "        N,F,OH,OW = A.shape\n",
    "        PH,PW = int(OH/self.P),int(OW/self.P)\n",
    "        self.params = N,F,OH,OW,self.P,PH,PW\n",
    "        self.PA = np.zeros([N,F,PH,PW])\n",
    "        self.Pindex = np.zeros([N,F,PH,PW])\n",
    "        for n in range(N):\n",
    "            for ch in range(F):\n",
    "                for row in range(PH):\n",
    "                    for col in range(PW):\n",
    "                        self.PA[n,ch,row,col] = np.max(A[n,ch,row*self.P:row*self.P+self.P,col*self.P:col*self.P+self.P])\n",
    "                        self.Pindex[n,ch,row,col] = np.argmax(A[n,ch,row*self.P:row*self.P+self.P,col*self.P:col*self.P+self.P])\n",
    "        return self.PA\n",
    "    \n",
    "    def backward(self,dA):\n",
    "        N,F,OH,OW,PS,PH,PW = self.params\n",
    "        dP = np.zeros([N,F,OH,OW])\n",
    "        for n in range(N): \n",
    "            for ch in range(F):\n",
    "                for row in range(PH):\n",
    "                    for col in range(PW):\n",
    "                        idx = self.Pindex[n,ch,row,col]\n",
    "                        tmp = np.zeros((PS*PS))\n",
    "                        for i in range(PS*PS):\n",
    "                            if i == idx:\n",
    "                                tmp[i] = dA[n,ch,row,col]\n",
    "                            else:\n",
    "                                tmp[i] = 0\n",
    "                        dP[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS] = tmp.reshape(PS,PS)\n",
    "        return dP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0377a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,X):\n",
    "        self.shape = X.shape\n",
    "        return X.reshape(len(X),-1)\n",
    "    def backward(self,X):\n",
    "        return X.reshape(self.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3f96ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# argmaxはFlattenされた後のindex\n",
    "np.argmax(np.array([[1,5], [3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aef16500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scratch2dCNNClassifier():\n",
    "    def __init__(self, NN, CNN, n_epoch=5, n_batch=1, verbose = False):\n",
    "        self.NN = NN\n",
    "        self.CNN = CNN\n",
    "        self.n_epoch = n_epoch\n",
    "        self.n_batch = n_batch\n",
    "        self.verbose = verbose\n",
    "        self.log_loss = np.zeros(self.n_epoch)\n",
    "        self.log_acc = np.zeros(self.n_epoch)\n",
    "    \n",
    "    def loss_function(self,y,yt):\n",
    "        delta = 1e-7\n",
    "        return -np.mean(yt*np.log(y+delta))\n",
    "\n",
    "    def accuracy(self,Z, Y):\n",
    "        return accuracy_score(Y, Z)\n",
    "    \n",
    "    def fit(self, X, y, X_val=False, y_val=False):\n",
    "        for epoch in range(self.n_epoch):\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.n_batch)\n",
    "            self.loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:              \n",
    "                forward_data = mini_X_train[:,np.newaxis,:,:]\n",
    "                for layer in range(len(self.CNN)):\n",
    "                    forward_data = self.CNN[layer].forward(forward_data)\n",
    "                flt = Flatten()\n",
    "                forward_data = flt.forward(forward_data)\n",
    "                for layer in range(len(self.NN)):\n",
    "                    forward_data = self.NN[layer].forward(forward_data)\n",
    "\n",
    "                Z = forward_data\n",
    "                backward_data = (Z - mini_y_train)/self.n_batch\n",
    "                for layer in range(len(self.NN)-1,-1,-1):\n",
    "                    backward_data = self.NN[layer].backward(backward_data)\n",
    "                backward_data = flt.backward(backward_data)\n",
    "                for layer in range(len(self.CNN)-1,-1,-1):\n",
    "                    backward_data = self.CNN[layer].backward(backward_data)\n",
    "\n",
    "                self.loss += self.loss_function(Z,mini_y_train)\n",
    "                if self.verbose:\n",
    "                    print('batch loss %f'%self.loss_function(Z,mini_y_train))\n",
    "            if self.verbose:\n",
    "                print(self.loss/len(get_mini_batch),self.accuracy(self.predict(X),np.argmax(y,axis=1)))\n",
    "            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n",
    "            self.log_acc[epoch] = self.accuracy(self.predict(X),np.argmax(y,axis=1))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        pred_data = X[:,np.newaxis,:,:]\n",
    "        flt = Flatten()\n",
    "        for layer in range(len(self.CNN)):\n",
    "            pred_data = self.CNN[layer].forward(pred_data)\n",
    "        pred_data = flt.forward(pred_data)\n",
    "        for layer in range(len(self.NN)):\n",
    "            pred_data = self.NN[layer].forward(pred_data)\n",
    "        return np.argmax(pred_data,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0797430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = {\n",
    "    0: FC(1960, 200, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "    1: FC(200, 200, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "    2: FC(200, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),\n",
    "}\n",
    "CNN = {\n",
    "    0: SimpleConv2d(F=10, C=1, FH=3, FW=3, P=1, S=1,initializer=SimpleInitializerConv2d(),optimizer=SGD(0.01),activation=ReLU()),\n",
    "    1: MaxPool2D(2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12d41120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss 0.230400\n",
      "batch loss 0.230453\n",
      "batch loss 0.230169\n",
      "batch loss 0.229982\n",
      "batch loss 0.230353\n",
      "batch loss 0.230628\n",
      "batch loss 0.230767\n",
      "batch loss 0.230394\n",
      "batch loss 0.230900\n",
      "batch loss 0.230146\n",
      "batch loss 0.229920\n",
      "batch loss 0.237231\n",
      "batch loss 0.284944\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "batch loss nan\n",
      "nan 0.105\n"
     ]
    }
   ],
   "source": [
    "cnn2 = Scratch2dCNNClassifier(NN=NN,CNN=CNN,n_epoch=1,n_batch=20,verbose=True)\n",
    "cnn2.fit(X_train[:2000], y_train_one_hot[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e929266b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.097\n"
     ]
    }
   ],
   "source": [
    "y_pred = cnn2.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print('accuracy:{:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fde771",
   "metadata": {},
   "source": [
    "CNNの代表的な構造\n",
    "\n",
    "\n",
    "- LeNet\n",
    "\n",
    "1. 畳み込み層　出力チャンネル数6、フィルタサイズ5×5、ストライド1\n",
    "2. ReLU\n",
    "3. 最大プーリング\n",
    "4. 畳み込み層　出力チャンネル数16、フィルタサイズ5×5、ストライド1\n",
    "5. ReLU\n",
    "6. 最大プーリング\n",
    "7. 平滑化\n",
    "8. 全結合層　出力ノード数120\n",
    "9. ReLU\n",
    "10. 全結合層　出力ノード数84\n",
    "11. ReLU\n",
    "12. 全結合層　出力ノード数10\n",
    "13. ソフトマックス関数\n",
    "\n",
    "\n",
    "- AlexNet\n",
    "\n",
    "- VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b06ef",
   "metadata": {},
   "source": [
    "#### フィルタサイズ\n",
    "\n",
    "2次元畳み込み層において現在では3×3と1×1の使用が大半\n",
    "\n",
    "- 7×7などの大きめのものではなく、3×3のフィルタが一般的に使われる理由\n",
    "\n",
    "- 高さや幅方向を持たない1×1のフィルタの効果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
